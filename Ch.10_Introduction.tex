\chapter{Introduction\label{intro}}

The integration of artificial intelligence (AI) into social engineering practices presents exceptional threats to privacy and security, necessitating a truly comprehensive re-evaluation, and enchantment, of current cybersecurity measures. Presenting both novel threats, as well as novel learning opportunities as countermeasures for AI attacks.
Cybersecurity is often broken down into three parts, representing the CIA Triad -model[1], which stands for Confidentiality, Integrity, and Availability. Future needs have necessitated that Auditability and Controllability be added to these. These three parts are the core principles designed to guide information security policies and practices within an organization. Let’s take a look at each of these next.
Confidentiality, which aims to protect sensitive information from unauthorized access or disclosure to 3rd parties. It ensures that data is accessible only to those who have the right to access it. Techniques like encryption, access control methods and authentication mechanisms are used to maintain confidentiality of data.
Integrity involves maintaining the accuracy and completeness of data. It ensures that information is not altered or tampered with by unauthorized individuals, and also aims to safeguard data against accidental alteration or deletion. Mechanisms to ensure integrity include checksum calculations (for example SHA-128, MD5), digital signatures and certificates (ex. TLS, SSL), and audit trails (usage access).
Availability ensures that information and digital resources are available to authorized users when needed. It focuses on preventing and mitigating service disruptions due to hardware failures, cyberattacks (Denial-of-Service attacks), and natural disasters (power outages, flooding, earthquakes and hurricanes). Redundancy, failover systems and regular maintenance are strategies used to ensure the necessary availability of data. Data that is on a hard drive on the moon is quite safe, but it’s useless because it can’t be accessed easily, if at all.
Controllability refers to a system’s ability to achieve a desired outcome, in the designated time, via control signals or other means.
Auditability refers to the ability for a system’s state, past in particular, to be read. No logs have been deleted.

\begin{enumerate}
    \item With the previous in mind, it is often said that the most secure computer is the one that hasn’t been plugged in to the powerline and turned on. However, with social engineering, an attacker could convince their target to plug that computer in and turn it on. While having effective, up-to-date and maintained solutions against such common threats to corporate/individual cybersecurity as viruses, crackers (hackers) and botnet infestations is absolutely vital, so too is the constant training and vigilance of the employees or the individual. Ultimately, it is the end-user who is the weakest link in the cybersecurity tree, and yet it’s surprisingly common for corporations to under allocate their cybersecurity budget to social engineering training and awareness.
\end{enumerate}
\begin{itemize}
    \item Social engineering (referred to as SE from now on) is the art, and science, of manipulation of the end-users via the exploits of human psychology, rather than technical hacking methods, to gain access to buildings, systems, funds, or data. The goal of the social engineer, who are a subgroup of hackers, is often to deceive individuals into disclosing confidential information or performing actions that may compromise security.  SE leverages the natural tendency of people to trust (SOURCE). It’s based on the principle that it’s easier to exploit natural inclination to trust and help than it is to discover ways to hack the corporation’s computer systems. It doesn’t matter if the company has the most expensive and trusted firewalls and antivirus software if a simple phone call can result in the leakage of vital business data. Many sources define social engineering as merely the act of gaining information via psychological exploits, but clearly SE also includes acts where the victim is coerced into performing actions as well, such as holding a door open.
\end{itemize}
Social engineering attacks usually begin with what’s known as intelligence gathering, or Open-Source Intelligence (OSINT). It refers to the process of collecting and analysing information from publicly available sources such as from the Internet, traditional mass media (TV, newspapers, magazines), specialized journals and, perhaps especially, from social networking sites (LinkedIn, Facebook, Twitter). The goal is to find intelligence to inform decision-making and to help in drafting an attack plan. The key characteristic of OSINT is that it exclusively relies on publicly accessibly information, i.e. no hacking is done yet.
Gathered intelligence is then used to launch the attacks. There are multiple types of SE attacks. Let’s review the most common ones.
Phishing involves sending emails that appear to be from reputable sources to induce individuals to reveal personal information, such as passwords and credit card numbers. 
Spear phishing is a more targeted version of phishing where the attacker chooses specific individuals or enterprises and customizes their messages based on characteristics, job positions, and contracts, to make their attack less noticeable.
Pretexting involves creating a fabricated scenario, or “pretext”, to engage a targeted victim in a manner that increases the chance of divulging information.
Tailgating is where an attacker seeking entry to a restricted area secured by unattended electronic access control, for an example via a radio-frequency identifier badge (RFID) or a regular keypad, simply follows someone in who has legitimate access. They could, for an example, carry something heavy and simply ask for someone to “keep the door open” for them.
The recent, massive advances in the field of artificial intelligence significantly amplify the threat to traditional cybersecurity, but perhaps even more so in social engineering. Modern AI-based social engineering attacks are more personalized, automated, and scalable, making them significantly more effective and harder to detect than traditional methods. Let’s review some attacks that have already been carried out.
Phishing with AI-generated content: AI can be used to craft highly convincing and personalized phishing emails or messages that mimic the style and tone of communication from trusted sources. By automatically analysing vast amounts of data from social media, corporate websites and leaked databases, AI algorithms can generate personalized messages that are more likely to deceive the recipient.
Deepfakes for impersonation: Deepfake technology, in which AI is used to create realistic but entirely fabricated images, videos or audio recordings, can be employed to impersonate individuals. Deepfake is a portmanteau of “deep learning”, a common machine learning method, and “fake”. Attackers could create deepfake videos of voice recordings from the CEO or other executives, government officials or trusted figures to issue fraudulent instructions. In as early as 2017, researchers at the University of Washington, in collaboration with BuzzFeed, created a deepfake video of president Barack Obama insulting Donald Trump[3]. This video was created to increase awareness of the potential risks of AI deepfake technology.
Real-time audio and video manipulation: Emerging AI technologies enable real-time voice and video manipulation, allowing attackers to impersonate individuals in live communications. These could be used in phone or video calls or Zoom/Teams conferences to trick victims into believing they are interacting with a trusted individual. In 2019, scammers allegedly used AI voice morphing technology to impersonate the boss of a UK-based firm’s CEO, demanding a transfer of €220,000 over the phone[2]. So called “robocalls” could be the next big thing in automated attacks against a mass of people. Robocallers could even build a relationship with their target in order to extract money.
Automated social engineering: AI can automate the process of identifying targets, crafting messages and managing interactions. Bots powered by AI can engage multiple victims simultaneously via email, phone or video calls, across multiple platforms, and adapt their strategies based on the responses they receive from each target to improve their chances of success, or to aid the human social engineer in his/her work.
It is clear that the advancements in AI technology bring with it a lot of good but as is the case with any tools, they can also be used for malicious purposes. Going forward, increased employee training and social engineering awareness, now with modern AI threats, is absolutely vital for continued business operations. Luckily, AI is a double-edged sword; where it can be used to attack and harm, it can also be used as it was originally designed, to help. End-users could be trained in the threat of AI via the help of AI: helping users defend against itself.
What, if anything, can the end-user trust anymore?

de Oliveira Albuquerque, R., Villalba, L., Orozco, A., Buiati, F., & Kim, T.-H. (2014). A Layered Trust Information Security Architecture. Sensors, 14(12), 22754–22772. https://doi.org/10.3390/s141222754 
Incident 200: Fraudsters used AI to mimic voice of a UK-based firm’s CEO’s boss. (2019). Incidentdatabase.ai. Retrieved 2024-03-02 from https://incidentdatabase.ai/cite/200/ 
Incident 39: Deepfake Obama Introduction of Deepfakes. (2018, April 2). Incidentdatabase.ai. Retrieved 2024-03-02 from https://incidentdatabase.ai/cite/39 